{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a34e812-6333-48b2-9a70-bcc718df054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, logging\n",
    "\n",
    "def setup_logging(level=logging.INFO, to_stdout=True, include_name=True):\n",
    "    fmt = \"[%(asctime)s] %(levelname)s: \"\n",
    "    if include_name:\n",
    "        fmt += \"%(name)s: \"\n",
    "    fmt += \"%(message)s\"\n",
    "\n",
    "    handler = logging.StreamHandler(sys.stdout if to_stdout else sys.stderr)\n",
    "    handler.setFormatter(logging.Formatter(fmt))\n",
    "\n",
    "    root = logging.getLogger()\n",
    "    root.handlers.clear()\n",
    "    root.setLevel(level)\n",
    "    root.addHandler(handler)\n",
    "\n",
    "    logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"qdrant_client\").setLevel(logging.INFO)\n",
    "    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "logger = logging.getLogger(__name__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef34b63-20f8-41df-b8c2-ac9ea499e7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:39:19,796] INFO: __main__: CUDA available: True\n",
      "[2025-08-25 11:39:19,832] INFO: __main__: GPU name: NVIDIA A100 80GB PCIe\n",
      "[2025-08-25 11:39:19,835] INFO: __main__: FlashAttention2 available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2175/2586307826.py:57: UserWarning: Api key is used with an insecure connection.\n",
      "  self.client = QdrantClient(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:39:20,171] INFO: __main__: Qdrant client initialized with URL: http://qdrant:6333\n",
      "[2025-08-25 11:39:20,172] INFO: __main__: Loading retrieval model: vidore/colqwen2.5-v0.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26969a744af84620b5507a4b61c3ade0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6450d22bf2e349cb98d628f05dc9f31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:39:28,808] INFO: __main__: Retrieval loaded in 8.63s\n",
      "[2025-08-25 11:39:28,810] INFO: __main__: Processing PDF: sample1.pdf\n",
      "[2025-08-25 11:39:28,811] INFO: __main__: Converting PDF to images: sample1.pdf (dpi=150)\n",
      "[2025-08-25 11:39:29,887] INFO: __main__: pdf2image(sample1.pdf): 26 pages in 1.07s\n",
      "[2025-08-25 11:39:29,887] INFO: __main__: Added 26 pages from sample1.pdf\n",
      "[2025-08-25 11:39:29,888] INFO: __main__: Processing PDF: sample2.pdf\n",
      "[2025-08-25 11:39:29,888] INFO: __main__: Converting PDF to images: sample2.pdf (dpi=150)\n",
      "[2025-08-25 11:40:17,586] INFO: __main__: pdf2image(sample2.pdf): 256 pages in 47.70s\n",
      "[2025-08-25 11:40:17,588] INFO: __main__: Added 256 pages from sample2.pdf\n",
      "[2025-08-25 11:40:17,589] INFO: __main__: Creating embeddings for 282 pages from 2 PDFs... (batch_size=16, num_workers=8, prefetch_factor=2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):   0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:23,310] INFO: __main__: [batch 000] bs=16 data_wait=0.001s compute=2.971s\n",
      "[2025-08-25 11:40:23,312] INFO: __main__: first_batch_total_latency=2.972s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):   6%|▌         | 1/18 [00:05<01:37,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:24,381] INFO: __main__: [batch 001] bs=16 data_wait=0.001s compute=1.066s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  11%|█         | 2/18 [00:06<00:47,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:25,659] INFO: __main__: [batch 002] bs=16 data_wait=0.000s compute=1.274s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  17%|█▋        | 3/18 [00:08<00:33,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:26,938] INFO: __main__: [batch 003] bs=16 data_wait=0.000s compute=1.276s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  22%|██▏       | 4/18 [00:09<00:25,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:28,216] INFO: __main__: [batch 004] bs=16 data_wait=0.000s compute=1.275s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  28%|██▊       | 5/18 [00:10<00:21,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:29,496] INFO: __main__: [batch 005] bs=16 data_wait=0.000s compute=1.277s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  33%|███▎      | 6/18 [00:11<00:18,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:30,778] INFO: __main__: [batch 006] bs=16 data_wait=0.000s compute=1.278s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  39%|███▉      | 7/18 [00:13<00:15,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:32,060] INFO: __main__: [batch 007] bs=16 data_wait=0.000s compute=1.279s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  44%|████▍     | 8/18 [00:14<00:13,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:33,342] INFO: __main__: [batch 008] bs=16 data_wait=0.000s compute=1.279s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  50%|█████     | 9/18 [00:15<00:12,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:34,625] INFO: __main__: [batch 009] bs=16 data_wait=0.000s compute=1.280s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  56%|█████▌    | 10/18 [00:17<00:10,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:35,907] INFO: __main__: [batch 010] bs=16 data_wait=0.000s compute=1.280s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  61%|██████    | 11/18 [00:18<00:09,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:37,191] INFO: __main__: [batch 011] bs=16 data_wait=0.000s compute=1.281s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  67%|██████▋   | 12/18 [00:19<00:07,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:38,475] INFO: __main__: [batch 012] bs=16 data_wait=0.000s compute=1.282s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  72%|███████▏  | 13/18 [00:20<00:06,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:39,760] INFO: __main__: [batch 013] bs=16 data_wait=0.000s compute=1.281s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  78%|███████▊  | 14/18 [00:22<00:05,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:41,044] INFO: __main__: [batch 014] bs=16 data_wait=0.000s compute=1.281s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  83%|████████▎ | 15/18 [00:23<00:03,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:42,332] INFO: __main__: [batch 015] bs=16 data_wait=0.001s compute=1.285s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  89%|████████▉ | 16/18 [00:24<00:02,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:43,619] INFO: __main__: [batch 016] bs=16 data_wait=0.001s compute=1.284s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt):  94%|█████████▍| 17/18 [00:26<00:01,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:44,655] INFO: __main__: [batch 017] bs=10 data_wait=0.001s compute=1.034s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding pages (opt): 100%|██████████| 18/18 [00:27<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:44,660] INFO: __main__: throughput: 10.42 images/sec, total=282\n",
      "[2025-08-25 11:40:44,661] INFO: __main__: Created embeddings for 282 pages from 2 PDFs\n",
      "[2025-08-25 11:40:44,671] INFO: __main__: Deleting existing collection: multi_pdf_documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:40:44,877] INFO: __main__: Created collection multi_pdf_documents with multivector support\n",
      "[2025-08-25 11:40:50,487] INFO: __main__: Qdrant upsert 256 pts in 3.37s\n",
      "[2025-08-25 11:40:51,244] INFO: __main__: Qdrant upsert 26 pts in 0.34s\n",
      "[2025-08-25 11:40:51,245] INFO: __main__: Stored total 282 embeddings in Qdrant\n",
      "[2025-08-25 11:40:51,289] INFO: __main__: All embeddings stored in Qdrant successfully\n",
      "[2025-08-25 11:40:51,534] INFO: __main__: Loading VLM model: Qwen/Qwen2.5-VL-32B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1958c2a9522447899c11ce5d3b211752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-25 11:41:06,079] INFO: __main__: VLM loaded in 14.54s\n",
      "[2025-08-25 11:41:06,080] INFO: __main__: ==== 前処理完了 ====\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers.utils.import_utils import is_flash_attn_2_available\n",
    "from colpali_engine.models import ColQwen2_5, ColQwen2_5_Processor\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, Qwen2_5_VLProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import (\n",
    "    Distance, VectorParams, PointStruct,\n",
    "    MultiVectorConfig, MultiVectorComparator\n",
    ")\n",
    "\n",
    "def log_env():\n",
    "    cuda = torch.cuda.is_available()\n",
    "    logger.info(f\"CUDA available: {cuda}\")\n",
    "    if cuda:\n",
    "        logger.info(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    logger.info(f\"FlashAttention2 available: {is_flash_attn_2_available()}\")\n",
    "\n",
    "# ---------- Qdrant----------\n",
    "class QdrantVectorStore:\n",
    "    def __init__(\n",
    "        self,\n",
    "        url: str = \"http://qdrant:6333\",\n",
    "        api_key: Optional[str] = None,\n",
    "        collection_name: str = \"colpali_documents\",\n",
    "        timeout: float = 120.0,\n",
    "        prefer_grpc: bool = True,\n",
    "        grpc_port: int = 6334,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        QdrantVectorStore の初期化\n",
    "\n",
    "        Args:\n",
    "            url (str): Qdrant サーバーのURL (デフォルト: http://qdrant:6333)\n",
    "            api_key (Optional[str]): APIキー\n",
    "            collection_name (str): コレクション名。RDBのテーブルのような概念\n",
    "            timeout (float): タイムアウト秒数\n",
    "            prefer_grpc (bool): RESTよりgRPCを優先して通信する（バイナリデータの転送で、高速・低レイテンシ）\n",
    "            grpc_port (int): gRPCポート番号 (デフォルト: 6334)\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        #QdrantClientのインスタンス化\n",
    "        self.client = QdrantClient(\n",
    "            url=url,\n",
    "            api_key=api_key,\n",
    "            timeout=timeout,\n",
    "            prefer_grpc=prefer_grpc,\n",
    "            grpc_port=grpc_port,\n",
    "        )\n",
    "        logger.info(f\"Qdrant client initialized with URL: {url}\")\n",
    "\n",
    "    def create_collection(\n",
    "        self,\n",
    "        multivector_size: int = 256,\n",
    "        force_recreate: bool = True,\n",
    "        distance: Distance = Distance.COSINE,\n",
    "        comparator: MultiVectorComparator = MultiVectorComparator.MAX_SIM,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Qdrantにコレクションを作成するメソッド（マルチベクター対応）\n",
    "\n",
    "        Args:\n",
    "            multivector_size: 埋め込みモデルが出力するベクトルの次元数\n",
    "            force_recreate: Trueの場合、既存コレクションを削除して再作成\n",
    "            Distance.COSINE: 検索にコサイン類似度を使用\n",
    "            comparator(MultiVectorComparator.MAX_SIM): 複数のベクトルの中で一番スコアが高い(類似度が高い)ものを選ぶ\n",
    "\n",
    "        Raises:\n",
    "            Exception: コレクション作成時にエラーが発生した場合\n",
    "        \"\"\"\n",
    "        try:\n",
    "            exists = any(\n",
    "                col.name == self.collection_name\n",
    "                for col in self.client.get_collections().collections\n",
    "            )\n",
    "            if exists:\n",
    "                if force_recreate:\n",
    "                    logger.info(f\"Deleting existing collection: {self.collection_name}\")\n",
    "                    self.client.delete_collection(self.collection_name)\n",
    "                else:\n",
    "                    logger.info(f\"Collection {self.collection_name} already exists\")\n",
    "                    return\n",
    "                    \n",
    "            #コレクション作成メソッドの呼び出し\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors_config=VectorParams(\n",
    "                    size=multivector_size,\n",
    "                    distance=distance,\n",
    "                    multivector_config=MultiVectorConfig(comparator=comparator),\n",
    "                ),\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"Created collection {self.collection_name} with multivector support\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating collection: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _to_multivector(self, embedding: Any) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Qdrantにマルチベクターを登録する前処理\n",
    "        Embeddingの形状が1次元でも2次元でも、2次元リストに正規化する\n",
    "        \"\"\"\n",
    "        arr = (\n",
    "            #embeddingがTensorならCPUに移動し、fp32に変換したうえでNumPy配列にする\n",
    "            #Tensorでない場合、NumPy配列に変換し、精度をfp32に統一\n",
    "            embedding.detach().cpu().to(torch.float32).numpy()\n",
    "            if isinstance(embedding, torch.Tensor)\n",
    "            else np.array(embedding, dtype=np.float32)\n",
    "        )\n",
    "        #1次元のベクトルの場合[0.1, 0.2]、 2次元リストにラップして返す [[0.1, 0.2]]\n",
    "        if arr.ndim == 1:\n",
    "            return [arr.tolist()]\n",
    "        #2次元ベクトルの場合、そのままのpythonリストに変換して返す\n",
    "        if arr.ndim == 2:\n",
    "            return arr.tolist() \n",
    "        raise ValueError(f\"Unexpected embedding shape: {arr.shape}\")\n",
    "\n",
    "    def store_embeddings(\n",
    "        self,\n",
    "        embeddings: List[torch.Tensor],\n",
    "        metadata: List[Dict[str, Any]],\n",
    "        upsert_batch_size: int = 64,\n",
    "        wait: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Qdrantへembeddingをアップサート\n",
    "        \"\"\"\n",
    "        #embeddingsとmetadataの数が一致しているかをチェック\n",
    "        assert len(embeddings) == len(metadata), \"embeddings/metadata length mismatch\"\n",
    "\n",
    "        sent = 0\n",
    "        buf: List[PointStruct] = []\n",
    "\n",
    "        def flush(_points: List[PointStruct]):\n",
    "            \"\"\"\n",
    "            バッファに溜まったポイントをQdrantに送信する内部関数\n",
    "            ポイントはQdrantの基本単位で、1件のベクトルデータを表すレコード(id,ベクトル,メタデータ)。\n",
    "            バッファは複数のポイントを一時的に貯めておくリスト\n",
    "            \"\"\"\n",
    "            if not _points:\n",
    "                return\n",
    "            t0 = time.perf_counter()\n",
    "\n",
    "            # バッファに溜まったポイントをQdrantに一括送信\n",
    "            self.client.upsert(\n",
    "                collection_name=self.collection_name,\n",
    "                points=_points,\n",
    "                wait=wait, #False: WAL(ログ)への追記が終わった時点で次のバッファを送る → データファイルへの書き込み、インデックス更新はQdrant内で非同期処理\n",
    "            )\n",
    "            dt = time.perf_counter() - t0\n",
    "            logger.info(f\"Qdrant upsert {len(_points)} pts in {dt:.2f}s\")\n",
    "\n",
    "        try:\n",
    "            # embeddingとmetadataを1件ずつ処理\n",
    "            for i, (emb, meta) in enumerate(zip(embeddings, metadata)):\n",
    "                # embeddingを2次元リストに正規化\n",
    "                multivector = self._to_multivector(emb)\n",
    "                # Qdrantに送信可能な形式に変換。（id, ベクトル, メタデータ)=ポイント\n",
    "                pid = int(meta.get(\"global_page_num\", i))\n",
    "                buf.append(PointStruct(id=pid, vector=multivector, payload=meta))\n",
    "                # バッファがbatch_sizeに達したらQdrant送信\n",
    "                if len(buf) >= upsert_batch_size:\n",
    "                    flush(buf)\n",
    "                    sent += len(buf)\n",
    "                    buf = []\n",
    "            # ループ終了後に、余ったバッファがあれば送信\n",
    "            if buf:\n",
    "                flush(buf)\n",
    "                sent += len(buf)\n",
    "            logger.info(f\"Stored total {sent} embeddings in Qdrant\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error storing embeddings: {e}\")\n",
    "            raise\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query_embedding: torch.Tensor,\n",
    "        top_k: int = 3,\n",
    "        score_threshold: Optional[float] = None,\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        検索クエリの埋め込みベクトルを、Qdrantが受け付ける形式（2次元リスト）に変換する。\n",
    "        Qdrantで類似検索を実行して結果を整形して返す。\n",
    "    \n",
    "        Args:\n",
    "            query_embedding: 検索クエリの埋め込みベクトル\n",
    "            top_k: 上位何件を返すか（デフォルト: 3）\n",
    "            score_threshold : 類似度スコアのしきい値（低スコアを除外）\n",
    "        \n",
    "        Notes:\n",
    "            Qdrant API呼び出し時の主なパラメータ:\n",
    "              - collection_name: 検索対象のコレクション名\n",
    "              - query: 検索クエリベクトル（正規化済み）\n",
    "              - limit: 上位件数 (top_k)\n",
    "              - score_threshold: 類似度スコアのしきい値\n",
    "              - with_payload=True: 結果にペイロード（メタデータ）を含める\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # クエリ埋め込みをQdrantに渡せる形（2次元リスト）に正規化\n",
    "            query = self._to_multivector(query_embedding)\n",
    "            # Qdrantへ検索リクエストを送信\n",
    "            res = self.client.query_points(\n",
    "                collection_name=self.collection_name,\n",
    "                query=query,\n",
    "                limit=top_k,\n",
    "                score_threshold=score_threshold,\n",
    "                with_payload=True,\n",
    "            )\n",
    "            # 検索結果（id,スコア,メタデータ）を整形してPythonの辞書リストに変換\n",
    "            out = [\n",
    "                {\"id\": p.id, \"score\": float(p.score), \"payload\": p.payload}\n",
    "                for p in res.points\n",
    "            ]\n",
    "            logger.info(f\"Found {len(out)} results\")\n",
    "            return out\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during search: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_collection_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Qdrantのコレクション情報を取得\n",
    "    \n",
    "        Returns:\n",
    "            - name: コレクション名\n",
    "            - vectors_count: 登録されているベクトルの総数\n",
    "            - points_count: 登録されているポイントの総数\n",
    "            - status: コレクションの状態\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Qdrantからコレクション情報を取得\n",
    "            info = self.client.get_collection(self.collection_name)\n",
    "\n",
    "            # 必要な情報を辞書にまとめて返す\n",
    "            return {\n",
    "                \"name\": self.collection_name,\n",
    "                \"vectors_count\": info.vectors_count,\n",
    "                \"points_count\": info.points_count,\n",
    "                \"status\": info.status,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting collection info: {e}\")\n",
    "            return {}\n",
    "\n",
    "# ----------MultiModalRAG----------\n",
    "class MultiModalRAG:\n",
    "    def __init__(\n",
    "        self,\n",
    "        retrieval_model_name: str = \"vidore/colqwen2.5-v0.2\", # 検索用埋め込みモデル\n",
    "        vlm_model_name: str = \"Qwen/Qwen2.5-VL-32B-Instruct\", # 回答生成に用いるVLMモデル\n",
    "        qdrant_url: str = \"http://qdrant:6333\", #QdrantサーバーのURL\n",
    "        qdrant_api_key: Optional[str] = None,\n",
    "        collection_name: str = \"colpali_documents\", # Qdrantコレクション名\n",
    "        use_qdrant: bool = True,\n",
    "        pdf_dpi: int = 150, # PDF→画像変換時の解像度\n",
    "        pdf_fmt: str = \"jpeg\", # PDF→画像変換時のフォーマット（jpegはI/O軽量）\n",
    "        use_pdftocairo: bool = True, # pdf画像変換で、pdftocairoを利用する(高品質なレンダリング)\n",
    "    ):\n",
    "        \"\"\"\n",
    "        MultiModalRAGクラスの初期化処理。\n",
    "        \"\"\"\n",
    "        self.retrieval_model_name = retrieval_model_name\n",
    "        self.vlm_model_name = vlm_model_name\n",
    "        self.use_qdrant = use_qdrant\n",
    "        self.pdf_dpi = pdf_dpi\n",
    "        self.pdf_fmt = pdf_fmt\n",
    "        self.use_pdftocairo = use_pdftocairo\n",
    "\n",
    "        self.retrieval_model = None\n",
    "        self.retrieval_processor = None\n",
    "\n",
    "        self.vlm_model = None\n",
    "        self.vlm_processor = None\n",
    "\n",
    "        self.vector_store = QdrantVectorStore(\n",
    "            url=qdrant_url,\n",
    "            api_key=qdrant_api_key,\n",
    "            collection_name=collection_name,\n",
    "            timeout=120.0,\n",
    "            prefer_grpc=True,\n",
    "            grpc_port=6334,\n",
    "        )\n",
    "\n",
    "        self.all_document_images: List[Image.Image] = [] # PDFから変換した画像リスト\n",
    "        self.all_document_metadata: List[Dict[str, Any]] = [] # 各画像に対応するメタデータ\n",
    "        self.all_document_embeddings: List[torch.Tensor] = [] # 各画像ベクトル\n",
    "        self.document_mapping: Dict[str, Dict[str, int]] = {} # PDF名ごとのページ範囲マッピング\n",
    "\n",
    "    # ---- モデル読み込み ----\n",
    "    def load_retrieval(self) -> None:\n",
    "        \"\"\"\n",
    "        検索用埋め込みモデル（ColQwen2.5）をロード\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading retrieval model: {self.retrieval_model_name}\")\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        # 検索用埋め込みモデルをロード（検索用）\n",
    "        self.retrieval_model = ColQwen2_5.from_pretrained(\n",
    "            self.retrieval_model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None, # FlashAttention2対応なら高速化を有効にする\n",
    "        ).eval()\n",
    "        # プロセッサをロード（入力画像/テキストを埋め込み入力形式に整形）\n",
    "        self.retrieval_processor = ColQwen2_5_Processor.from_pretrained(\n",
    "            self.retrieval_model_name,\n",
    "            use_fast=True\n",
    "        )\n",
    "        logger.info(f\"Retrieval loaded in {time.perf_counter() - t0:.2f}s\")\n",
    "\n",
    "    def load_vlm(self) -> None:\n",
    "        \"\"\"\n",
    "        回答生成に使用する視覚言語モデル（Qwen2.5-VL）とそのプロセッサをロード\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading VLM model: {self.vlm_model_name}\")\n",
    "        t0 = time.perf_counter()\n",
    "        # VLMモデル本体をロード（生成用）\n",
    "        self.vlm_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            self.vlm_model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "            attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else None, # FlashAttention2対応なら高速化を有効にする\n",
    "        ).eval()\n",
    "        \n",
    "        # 入力画像サイズの下限と上限を指定（ピクセル数ベース）\n",
    "        min_pixels = 256 * 28 * 28\n",
    "        max_pixels = 2048 * 28 * 28\n",
    "\n",
    "        # VLM用のプロセッサをロード（画像・テキストを入力形式に整形）\n",
    "        self.vlm_processor = Qwen2_5_VLProcessor.from_pretrained(\n",
    "            self.vlm_model_name,\n",
    "            min_pixels=min_pixels, \n",
    "            max_pixels=max_pixels,\n",
    "            use_fast=True\n",
    "        )\n",
    "        logger.info(f\"VLM loaded in {time.perf_counter() - t0:.2f}s\")\n",
    "\n",
    "    def pdf_to_images(self, pdf_path: str) -> List[Image.Image]:\n",
    "        \"\"\"\n",
    "        PDFファイルをページごとに画像へ変換する関数\n",
    "\n",
    "        Args:\n",
    "            pdf_path (str): 入力PDFファイルのパス\n",
    "            \n",
    "        Returns:\n",
    "            List[Image.Image]: 変換後の各ページ画像のリスト\n",
    "        \"\"\"\n",
    "        logger.info(f\"Converting PDF to images: {pdf_path} (dpi={self.pdf_dpi})\")\n",
    "        \n",
    "         # PDFファイルの存在チェック（なければ例外を投げる）\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "            \n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        # pdf2imageを使ってPDFを画像に変換\n",
    "        images = convert_from_path(\n",
    "            pdf_path,\n",
    "            dpi=self.pdf_dpi, # 解像度（dpi）\n",
    "            use_pdftocairo=self.use_pdftocairo, # pdftocairoを使うか（高速・正確)\n",
    "            fmt=self.pdf_fmt, #出力画像フォーマット（jpeg）\n",
    "        )\n",
    "        dt = time.perf_counter() - t0\n",
    "        logger.info(f\"pdf2image({os.path.basename(pdf_path)}): {len(images)} pages in {dt:.2f}s\")\n",
    "        return images\n",
    "\n",
    "    def process_multiple_pdfs(\n",
    "        self,\n",
    "        pdf_paths: List[str],\n",
    "        batch_size: int = 16,\n",
    "        num_workers: int = 8,\n",
    "        prefetch_factor: int = 2,\n",
    "        force_recreate_collection: bool = False,\n",
    "        upsert_batch_size: int = 256,\n",
    "        upsert_wait: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        複数のPDFファイルを読み込み、画像・メタデータ・ベクトルを生成して保持する\n",
    "        \n",
    "        Args:\n",
    "            pdf_paths (List[str]): 入力PDFファイルのパスリスト\n",
    "            batch_size (int): 埋め込み生成時のDataLoaderバッチサイズ\n",
    "            num_workers (int): DataLoaderで並列に使うワーカー数\n",
    "            prefetch_factor (int): 各ワーカーが先読みするバッチ数\n",
    "            force_recreate_collection (bool): TrueならQdrantコレクションを再作成\n",
    "            upsert_batch_size (int):  Qdrantへ送る“書き込みバッチ”の件数。store_embeddings() にそのまま引き継がれる。\n",
    "            upsert_wait (bool): False: WAL(ログ)への追記が終わった時点でレスポンス返却 → データファイルへの書き込みとインデックス更新はQdrant内で非同期処理\n",
    "        \"\"\"\n",
    "        if self.retrieval_model is None or self.retrieval_processor is None:\n",
    "            raise ValueError(\"Retrieval model not loaded. Call load_retrieval() first.\")\n",
    "\n",
    "        all_images: List[Image.Image] = []\n",
    "        all_metadata: List[Dict[str, Any]] = []\n",
    "        start_page = 0\n",
    "\n",
    "        for pdf_path in pdf_paths:\n",
    "            logger.info(f\"Processing PDF: {pdf_path}\")\n",
    "            # PDFファイルの存在確認（なければスキップ）\n",
    "            if not os.path.exists(pdf_path):\n",
    "                logger.warning(f\"PDF file not found: {pdf_path}\")\n",
    "                continue\n",
    "\n",
    "            # PDFをページごとに画像へ変換\n",
    "            images = self.pdf_to_images(pdf_path)\n",
    "            pdf_name = os.path.basename(pdf_path)\n",
    "\n",
    "            # ページ範囲を記録（ドキュメントマッピング用）\n",
    "            end_page = start_page + len(images)\n",
    "            self.document_mapping[pdf_name] = {\n",
    "                \"start_page\": start_page,\n",
    "                \"end_page\": end_page,\n",
    "                \"total_pages\": len(images),\n",
    "            }\n",
    "\n",
    "            # ページごとのメタデータを作成\n",
    "            metadata = [\n",
    "                {\n",
    "                    \"pdf_path\": pdf_path,\n",
    "                    \"pdf_name\": pdf_name,\n",
    "                    \"page_num\": i + 1, # PDF内のページ番号（1始まり）\n",
    "                    \"global_page_num\": start_page + i, # 全体での通しページ番号\n",
    "                    \"image_path\": f\"{pdf_name}_page_{i+1}.{self.pdf_fmt}\", # 変換後の画像ファイル名\n",
    "                }\n",
    "                for i in range(len(images))\n",
    "            ]\n",
    "\n",
    "            # 全体リストに追加(複数PDF分の変換データ)\n",
    "            all_images.extend(images)\n",
    "            all_metadata.extend(metadata)\n",
    "            start_page = end_page # 次のPDFのページ開始位置を更新\n",
    "            \n",
    "            logger.info(f\"Added {len(images)} pages from {pdf_name}\")\n",
    "\n",
    "        # PDFから画像に変換した結果をインスタンス変数に保存（後で埋め込み生成や検索に利用）\n",
    "        self.all_document_images = all_images\n",
    "        self.all_document_metadata = all_metadata\n",
    "\n",
    "        # 画像を埋め込みモデルの入力形式に変換するためのヘルパー関数\n",
    "        def collate_fn(batch_imgs: List[Image.Image]):\n",
    "            return self.retrieval_processor.process_images(batch_imgs)\n",
    "\n",
    "        # DataLoaderの設定（画像を埋め込みモデルに効率的に流し込み）\n",
    "        dl_kwargs = dict(\n",
    "            batch_size=max(1,batch_size),\n",
    "            shuffle=False, # 順序を固定（PDFのページ順を維持）\n",
    "            collate_fn=collate_fn, # 画像を埋め込みモデルの入力形式に変換する前処理関数\n",
    "            num_workers=num_workers, # データローダーのワーカー数\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "            persistent_workers=(num_workers > 0), # ワーカーを持続させる（エポックごとにワーカーを再作成しない）\n",
    "        )\n",
    "        if num_workers > 0:\n",
    "            dl_kwargs[\"prefetch_factor\"] = prefetch_factor # GPUに渡す次バッチを、CPUワーカーが先に用意しておく深さ。GPUを待たせないため・\n",
    "\n",
    "        dataloader = DataLoader(all_images, **dl_kwargs)\n",
    "\n",
    "        logger.info(\n",
    "            f\"Creating embeddings for {len(all_images)} pages from {len(pdf_paths)} PDFs...\"\n",
    "            f\" (batch_size={dl_kwargs['batch_size']}, num_workers={num_workers}\"\n",
    "            f\", prefetch_factor={prefetch_factor if num_workers>0 else 'n/a'})\"\n",
    "        )\n",
    "\n",
    "        # 行列演算の高速化設定（TF32を使った最適化）\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "        # 埋め込みを溜めるリスト\n",
    "        embeddings: List[torch.Tensor] = []\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        first_batch_logged = False\n",
    "\n",
    "        # 画像のベクトル変換\n",
    "        with torch.inference_mode(), torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            for bi, batch_images in enumerate(tqdm(dataloader, desc=\"Embedding pages (opt)\")):\n",
    "                t_start = time.perf_counter()\n",
    "                # GPUへ転送\n",
    "                batch_images = {\n",
    "                    k: v.to(self.retrieval_model.device, non_blocking=True)\n",
    "                    for k, v in batch_images.items()\n",
    "                }\n",
    "                t_ready = time.perf_counter()\n",
    "\n",
    "                # 画像のベクトル変換\n",
    "                out = self.retrieval_model(**batch_images)\n",
    "                t_done = time.perf_counter()\n",
    "\n",
    "                # バッチ出力を1件ずつ分解してリストに追加\n",
    "                embeddings.extend(torch.unbind(out))\n",
    "\n",
    "                data_wait = t_ready - t_start  # データ準備にかかった時間\n",
    "                compute = t_done - t_ready  # 推論計算にかかった時間\n",
    "                logger.info(f\"[batch {bi:03d}] bs={out.shape[0]} data_wait={data_wait:.3f}s compute={compute:.3f}s\")\n",
    "                if not first_batch_logged:\n",
    "                    logger.info(f\"first_batch_total_latency={t_done - t_start:.3f}s\")\n",
    "                    first_batch_logged = True\n",
    "\n",
    "        # CUDA環境ならGPUの処理が終わるまで同期\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t1 = time.perf_counter()\n",
    "        total = len(embeddings)\n",
    "        logger.info(f\"throughput: {total / (t1 - t0):.2f} images/sec, total={total}\")\n",
    "        self.all_document_embeddings = embeddings\n",
    "        logger.info(f\"Created embeddings for {total} pages from {len(pdf_paths)} PDFs\")\n",
    "\n",
    "        # 画像ベクトルをQdrantへ保存\n",
    "        if self.use_qdrant and self.vector_store and total > 0:\n",
    "            emb_dim = embeddings[0].shape[-1] # ベクトルの次元数を取得\n",
    "            self.vector_store.create_collection(\n",
    "                multivector_size=emb_dim,\n",
    "                force_recreate=force_recreate_collection,\n",
    "            )\n",
    "            # 画像ベクトルとメタデータをQdrantにバッチ送信\n",
    "            self.vector_store.store_embeddings(\n",
    "                embeddings, all_metadata,\n",
    "                upsert_batch_size=upsert_batch_size, # Qdrantへの書き込みバッチ件数\n",
    "                wait=upsert_wait,\n",
    "            )\n",
    "            logger.info(\"All embeddings stored in Qdrant successfully\")\n",
    "\n",
    "    def search_documents(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        テキストクエリをベクトルに変換して Qdrantで類似検索し、検索結果を返す関数\n",
    "        \"\"\"\n",
    "        if self.retrieval_model is None or self.retrieval_processor is None:\n",
    "            raise ValueError(\"Retrieval model not loaded. Call load_retrieval() first.\")\n",
    "        # テキストクエリのベクトル変換\n",
    "        with torch.no_grad():\n",
    "            # クエリ文字列を前処理して、モデル入力形式に変換（トークナイズ）\n",
    "            proc = self.retrieval_processor.process_queries([query]).to(self.retrieval_model.device)\n",
    "            # 検索用埋め込みベクトルを生成\n",
    "            q_emb = self.retrieval_model(**proc)\n",
    "        # Qdrantに埋め込みを渡して類似検索を実行\n",
    "        hits = self.vector_store.search(q_emb[0], top_k=top_k)\n",
    "        results = []\n",
    "        # Qdrantの検索結果を整形\n",
    "        for h in hits:\n",
    "            meta = h[\"payload\"]\n",
    "            page_idx = int(meta.get(\"global_page_num\", h[\"id\"]))\n",
    "            results.append({\n",
    "                \"pdf_name\": meta[\"pdf_name\"],\n",
    "                \"pdf_path\": meta[\"pdf_path\"],\n",
    "                \"page_num\": meta[\"page_num\"],  # PDF内のページ番号（1始まり）\n",
    "                \"global_page_num\": meta[\"global_page_num\"],  # 全体ページ番号（通し番号）\n",
    "                \"score\": h[\"score\"],  # 類似度スコア\n",
    "                \"metadata\": meta,  # メタ情報\n",
    "                \"image\": self.all_document_images[page_idx] if 0 <= page_idx < len(self.all_document_images) else None, #画像\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def generate_answer(\n",
    "        self, question: str, search_results: List[Dict[str, Any]], max_new_tokens: int = 500\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        検索結果から画像を抽出して、ユーザーの質問と合わせてVLMに入力して回答を生成する\n",
    "        \"\"\"\n",
    "        if self.vlm_model is None or self.vlm_processor is None:\n",
    "            raise ValueError(\"VLM model not loaded. Call load_vlm() first.\")\n",
    "\n",
    "        # 検索結果から画像を抽出\n",
    "        imgs = [r[\"image\"] for r in search_results if r.get(\"image\") is not None]\n",
    "\n",
    "        # VLMに渡すチャット形式の入力を作成\n",
    "        chat_msgs = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                # 検索結果の画像を順番に追加\n",
    "                *[{\"type\": \"image\", \"image\": img} for img in imgs],\n",
    "                # ユーザー質問をテキストとして追加\n",
    "                {\"type\": \"text\", \"text\": f\"以下の画像を参照して質問に答えてください：\\n\\n質問: {question}\\n\\n回答:\"}\n",
    "            ]\n",
    "        }]\n",
    "\n",
    "        # モデルが解釈できるチャットテンプレートに整形\n",
    "        text = self.vlm_processor.apply_chat_template(\n",
    "            chat_msgs, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        # チャットメッセージから画像入力だけを抽出\n",
    "        img_inputs, _ = process_vision_info(chat_msgs)\n",
    "\n",
    "        # プロセッサでテキストをトークナイズして、画像を前処理してテンソル化\n",
    "        inputs = self.vlm_processor(\n",
    "            text=[text],\n",
    "            images=img_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.vlm_model.device)\n",
    "\n",
    "        # VLMで回答生成\n",
    "        with torch.no_grad():\n",
    "            gen_ids = self.vlm_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "            )\n",
    "\n",
    "        # プロンプトを除外し、出力部分のみテキストに変換して表示\n",
    "        trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, gen_ids)]\n",
    "        decoded = self.vlm_processor.batch_decode(\n",
    "            trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "        return decoded[0].strip()\n",
    "\n",
    "class AdvancedQASystem:\n",
    "    def __init__(self, rag: MultiModalRAG):\n",
    "        self.rag = rag\n",
    "\n",
    "    def answer_question(self, question: str, top_k: int = 3, max_new_tokens: int = 500) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ユーザーの質問に対して、検索と回答生成をまとめて実行する。\n",
    "        \n",
    "        Args:\n",
    "            question (str): ユーザーの質問（自然文）\n",
    "            top_k (int): 検索で取得する上位候補数\n",
    "            max_new_tokens (int): 回答生成の最大トークン数\n",
    "        \"\"\"\n",
    "        # 関連文書を検索\n",
    "        results = self.rag.search_documents(question, top_k=top_k)\n",
    "        \n",
    "        # 検索結果を基に回答を生成\n",
    "        answer = self.rag.generate_answer(question, results, max_new_tokens=max_new_tokens)\n",
    "\n",
    "        # 質問文・回答文・参照したソース情報（PDF名・ページ番号・スコア）を辞書で返す\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"sources\": [\n",
    "                {\"pdf_name\": r[\"pdf_name\"], \"page_num\": r[\"page_num\"], \"score\": r[\"score\"]}\n",
    "                for r in results\n",
    "            ],\n",
    "        }\n",
    "\n",
    "# ----------エントリーポイント----------\n",
    "if __name__ == \"__main__\":\n",
    "    setup_logging(level=logging.INFO, to_stdout=True, include_name=True)  \n",
    "    log_env()\n",
    "\n",
    "    # MultiModalRAG のインスタンスを初期化\n",
    "    rag = MultiModalRAG(\n",
    "        use_qdrant=True,\n",
    "        qdrant_url=\"http://qdrant:6333\",\n",
    "        qdrant_api_key=\"colqwen25-qdrant-api-key\",\n",
    "        collection_name=\"multi_pdf_documents\",\n",
    "        pdf_dpi=150, # PDF→画像変換時の解像度（DPI）\n",
    "        pdf_fmt=\"jpeg\",\n",
    "        use_pdftocairo=True,\n",
    "    )\n",
    "\n",
    "    # 検索用モデルのみロード\n",
    "    rag.load_retrieval()\n",
    "\n",
    "    # サンプルPDFファイルを指定\n",
    "    pdf_paths = [\"sample1.pdf\", \"sample2.pdf\"]\n",
    "    \n",
    "    existing_pdfs = [p for p in pdf_paths if os.path.exists(p)]\n",
    "    # 実際に存在するPDFだけを対象にする\n",
    "    if not existing_pdfs:\n",
    "        logger.warning(\"No input PDFs found. Put your PDFs next to this script.\")\n",
    "    else:\n",
    "        # PDF群 → 画像変換 → 埋め込み生成 → Qdrantに保存\n",
    "        rag.process_multiple_pdfs(\n",
    "            existing_pdfs,\n",
    "            batch_size=16,           # DataLoaderのバッチサイズ\n",
    "            num_workers=8,            # DataLoaderの並列ワーカー数（CPUリソースに依存）\n",
    "            prefetch_factor=2,        # ワーカーごとの先読みバッチ数\n",
    "            force_recreate_collection=True, # 既存コレクションを削除して再作成\n",
    "            upsert_batch_size=256,    # Qdrantに送信するバッチサイズ\n",
    "            upsert_wait=False,        # Trueならアップサート完了を待つ\n",
    "        )\n",
    "\n",
    "        # 回答生成（VLM）が必要になった段階でロード\n",
    "        rag.load_vlm()\n",
    "\n",
    "        # QAシステムをラップするクラスを用意\n",
    "        qa = AdvancedQASystem(rag)\n",
    "\n",
    "        logger.info(\"==== 前処理完了 ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10702800-788c-473f-bd2d-d5c182e599f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 開始: 28件の質問を処理します ===\n",
      "\n",
      "================================================================================\n",
      "【1/28】質問: 液晶モニター文書の主な内容は何ですか？\n",
      "--------------------------------------------------------------------------------\n",
      "[2025-08-25 11:41:06,221] INFO: __main__: Found 5 results\n",
      "【回答】\n",
      "液晶モニター文書の主な内容は、以下の通りです：\n",
      "\n",
      "1. **製品紹介と安全確認**  \n",
      "   - LG製液晶モニター（IPS225VおよびIPS235V）の取扱説明書として、製品の使用前に必ず内容を読み、保管するよう案内しています。\n",
      "\n",
      "2. **目次**  \n",
      "   - 文書の構成を示しており、主な項目が以下のようにまとめられています：\n",
      "     - **組み立てと準備**: 同梱品の確認、各部の名称、モニターの組み立て手順、スタンドの取り付け・取り外し、設置方法（テーブルや壁への取り付け）。\n",
      "     - **モニターの接続**: PCへの接続方法（D-SUB、DVI-D、HDMI接続など）。\n",
      "     - **設定の方法**: メニューへのアクセス方法、画質調整、音量設定、モード選択、デュアルモニター設定などの操作手順。\n",
      "     - **トラブルシューティング**: 問題解決のためのアドバイス。\n",
      "     - **仕様**: 製品の仕様情報（解像度、電源ランプなど）。\n",
      "\n",
      "3. **各部の名称**  \n",
      "   - モニターの各部分を図\n",
      "\n",
      "【参照ソース】\n",
      "- sample1.pdf ページ1 (スコア:18.606)\n",
      "- sample1.pdf ページ2 (スコア:15.781)\n",
      "- sample2.pdf ページ4 (スコア:14.689)\n",
      "- sample1.pdf ページ12 (スコア:14.242)\n",
      "- sample1.pdf ページ4 (スコア:14.205)\n",
      "\n",
      "================================================================================\n",
      "【2/28】質問: メニューの図において、明るさ、コントラスト、画質のそれぞれの数値を教えて下さい。\n",
      "--------------------------------------------------------------------------------\n",
      "[2025-08-25 11:41:29,823] INFO: __main__: Found 5 results\n",
      "【回答】\n",
      "### 回答:\n",
      "\n",
      "メニューの図において、明るさ、コントラスト、画質のそれぞれの数値は以下の通りです：\n",
      "\n",
      "1. **明るさ (明るさ)**: 100  \n",
      "2. **コントラスト**: 70  \n",
      "3. **画質**: 5  \n",
      "\n",
      "これらの値は、画像中の「MENU → PICTURE」のOSDメニューで表示されています。\n",
      "\n",
      "【参照ソース】\n",
      "- sample1.pdf ページ13 (スコア:26.353)\n",
      "- sample1.pdf ページ14 (スコア:25.555)\n",
      "- sample1.pdf ページ15 (スコア:21.874)\n",
      "- sample1.pdf ページ16 (スコア:21.090)\n",
      "- sample1.pdf ページ17 (スコア:20.778)\n",
      "\n",
      "================================================================================\n",
      "【3/28】質問: モニターの角度について、何度に調整できますか？\n",
      "--------------------------------------------------------------------------------\n",
      "[2025-08-25 11:41:40,301] INFO: __main__: Found 5 results\n",
      "【回答】\n",
      "モニターの角度は、上下に \\(-5^\\circ\\) ～ \\(+15^\\circ\\) の範囲で調整できます。\n",
      "\n",
      "【参照ソース】\n",
      "- sample1.pdf ページ7 (スコア:20.780)\n",
      "- sample1.pdf ページ14 (スコア:17.717)\n",
      "- sample1.pdf ページ18 (スコア:17.679)\n",
      "- sample1.pdf ページ12 (スコア:17.621)\n",
      "- sample1.pdf ページ6 (スコア:17.615)\n",
      "\n",
      "================================================================================\n",
      "【4/28】質問: MODE設定の画面において、ムービーのアイコンは左から何番目にある？\n",
      "--------------------------------------------------------------------------------\n",
      "[2025-08-25 11:41:47,217] INFO: __main__: Found 5 results\n",
      "【回答】\n",
      "### 分析\n",
      "\n",
      "我们需要确定在 MODE 設定的画面上，**ムービー (Movie)** 的图标是左起第几个。以下是逐步分析：\n",
      "\n",
      "1. **MODE 設定の画面**:\n",
      "   - 画像中提供了 MODE 設定的菜单界面。\n",
      "   - 该界面上有五个图标，分别对应不同的模式选项。\n",
      "\n",
      "2. **图标的排列顺序**:\n",
      "   - 图标从左到右依次为：\n",
      "     1. ユーザー (User)\n",
      "     2. ムービー (Movie)\n",
      "     3. テキスト (Text)\n",
      "     4. 画像 (Image)\n",
      "     5. sRGB\n",
      "\n",
      "3. **目标图标的位置**:\n",
      "   - ムービー (Movie) 的图标是第二个图标。\n",
      "\n",
      "### 答案\n",
      "\n",
      "ムービーのアイコンは左から **2番目** にあります。\n",
      "\n",
      "**最終回答: \\boxed{2}**\n",
      "\n",
      "【参照ソース】\n",
      "- sample1.pdf ページ18 (スコア:22.572)\n",
      "- sample1.pdf ページ5 (スコア:20.350)\n",
      "- sample1.pdf ページ17 (スコア:17.755)\n",
      "- sample1.pdf ページ12 (スコア:17.198)\n",
      "- sample1.pdf ページ16 (スコア:16.918)\n",
      "\n",
      "================================================================================\n",
      "【5/28】質問: 動作温度は？\n",
      "--------------------------------------------------------------------------------\n",
      "[2025-08-25 11:42:05,023] INFO: __main__: Found 5 results\n",
      "【回答】\n",
      "### 質問: 動作温度は？\n",
      "### 回答:\n",
      "\n",
      "両方のモニター（IPS225V と IPS235V）の動作温度は、**10°C - 35°C** です。この情報は、画像 2 和 画像 3 に記載されています。\n",
      "\n",
      "- **IPS225V**: 動作温度は **10°C - 35°C**\n",
      "- **IPS235V**: 動作温度は **10°C - 35°C**\n",
      "\n",
      "したがって、両モデル共通の動作温度範囲は **10°C - 35°C** です。\n",
      "\n",
      "【参照ソース】\n",
      "- sample1.pdf ページ23 (スコア:13.517)\n",
      "- sample1.pdf ページ22 (スコア:13.506)\n",
      "- sample2.pdf ページ70 (スコア:9.933)\n",
      "- sample1.pdf ページ9 (スコア:9.916)\n",
      "- sample2.pdf ページ179 (スコア:9.892)\n",
      "\n",
      "================================================================================\n",
      "【6/28】質問: スタンドをモニターに接続する方法は？\n",
      "--------------------------------------------------------------------------------\n",
      "[2025-08-25 11:42:19,495] INFO: __main__: Found 5 results\n",
      "【回答】\n",
      "### 質問: スタンドをモニターに接続する方法は？\n",
      "\n",
      "### 回答:\n",
      "スタンドをモニターに接続する方法は、以下の手順に従って行います。\n",
      "\n",
      "1. **モニターの準備**  \n",
      "   - モニターの画面側を下にして、クッション性のある平面上置きます。  \n",
      "   - 画面に傷がつかないように、モニターを置く前にフォームマットや柔らかい保護布を敷いてください。\n",
      "\n",
      "2. **スタンドの取り付け**  \n",
      "   - スタンドを取り付け、硬貨などでネジを右に回して締め付けます。  \n",
      "\n",
      "   **注意事項:**  \n",
      "   - ネジをしっかりと締めて、スタンドがしっかり固定されていることを確認してください。\n",
      "   - モニターを持ち上げる際には、安全を確保し、指を挟む事故に注意してください。\n",
      "\n",
      "### 参考画像:\n",
      "![接続手順](https://i.imgur.com/XXXXXX.png)  \n",
      "（注：実際の画像は上記PDFからの参照画像です）\n",
      "\n",
      "以上がスタンドをモニターに接続する方法です。手順に従いながら、安全に作業を行ってください。\n",
      "\n",
      "【参照ソース】\n",
      "- sample1.pdf ページ2 (スコア:19.013)\n",
      "- sample1.pdf ページ6 (スコア:18.551)\n",
      "- sample1.pdf ページ10 (スコア:18.038)\n",
      "- sample1.pdf ページ8 (スコア:17.284)\n",
      "- sample1.pdf ページ7 (スコア:17.268)\n",
      "\n",
      "================================================================================\n",
      "【7/28】質問: モニター背面の接続端子の種類を教えて下さい。\n",
      "--------------------------------------------------------------------------------\n",
      "[2025-08-25 11:42:41,484] INFO: __main__: Found 5 results\n",
      "【回答】\n",
      "モニター背面の接続端子には、次の種類が用意されています。\n",
      "\n",
      "1. **D-SUB**  \n",
      "   - アナログ信号（RGB）を使用するための15ピンD-Subコネクタ。\n",
      "   - PCやその他のデバイスからのアナログ映像信号を受信します。\n",
      "\n",
      "2. **DVI-D**  \n",
      "   - デジタル信号を使用するためのDVI-Dコネクタ。\n",
      "   - PCやその他のデバイスからのデジタル映像信号を受信します。\n",
      "\n",
      "3. **HDMI**  \n",
      "   - デジタル信号（映像と音声）を使用するためのHDMIコネクタ。\n",
      "   - PCやA/V機器からのデジタル映像および音声信号を受信します。\n",
      "\n",
      "これらの端子は、PCや他のデバイスとの接続に使用されます。各端子の接続方法については、取扱説明書の詳細な図や記述を参照してください。\n",
      "\n",
      "【参照ソース】\n",
      "- sample1.pdf ページ10 (スコア:21.351)\n",
      "- sample1.pdf ページ4 (スコア:21.096)\n",
      "- sample1.pdf ページ2 (スコア:19.931)\n",
      "- sample1.pdf ページ11 (スコア:19.307)\n",
      "- sample1.pdf ページ7 (スコア:18.880)\n",
      "\n",
      "================================================================================\n",
      "【8/28】質問: 解像度の最小値と最大値は？\n",
      "--------------------------------------------------------------------------------\n",
      "[2025-08-25 11:43:00,040] INFO: __main__: Found 5 results\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"液晶モニター文書の主な内容は何ですか？\",\n",
    "    \"メニューの図において、明るさ、コントラスト、画質のそれぞれの数値を教えて下さい。\",\n",
    "    \"モニターの角度について、何度に調整できますか？\",\n",
    "    \"MODE設定の画面において、ムービーのアイコンは左から何番目にある？\",\n",
    "    \"動作温度は？\",\n",
    "    \"スタンドをモニターに接続する方法は？\",\n",
    "    \"モニター背面の接続端子の種類を教えて下さい。\",\n",
    "    \"解像度の最小値と最大値は？\",\n",
    "    \"スタンドを含めたモニタ全体の寸法は？\",\n",
    "    \"Kensingtonセキュリティケーブルはどのようにして固定していますか？\",        \n",
    "    \"令和７年情報通信白書の主な内容は何ですか？\",\n",
    "    \"2014年から2024年にかけてLINE利用率が最も成長した年代は？\",\n",
    "    \"2020年以降、2番目に高い利用率の動画サービスは？\",\n",
    "    \"ニュース目的のソーシャルメディア利用率において、全年代でトップのメディアは何？\",\n",
    "    \"海外プラットフォーム事業者の売上高で最も成長性しているのは？\",\n",
    "    \"日本における生成AI導入に際しての懸念事項で最も多いものは？\",\n",
    "    \"生成AI関連市場の市場構成図について解説してください。\",\n",
    "    \"DX・イノベーション加速プラン2030はどのような図を表していますか？\",\n",
    "    \"テレワークの導入形態においてサテライトオフィス勤務は何%になっていますか？\"\n",
    "    \"AIによるリスク例の体系的な分類案において、経済活動に関するリスクを挙げてください。\",\n",
    "    \"米国の民間情報化投資は日本と比較してどのように遷移していますか？\",\n",
    "    \"インターネット利用時に不安を感じないひとの割合は？\",\n",
    "    \"グローバルICT市場の国・地域別シェアの推計（カテゴリー別）（2023年）において、クラウドの米国シェアは？\",\n",
    "    \"海底ケーブルの使用帯域幅の推移において、コンテンツプロバイダーはどのように遷移していますか？\",\n",
    "    \"日本のデータセンターサービス市場規模（売上高）の推移予測において、前年比成長率はどのように遷移していますか？\",\n",
    "    \"世界のICT市場における時価総額上位15社の変遷において、2025年に向上した企業を教えてもらえますか？\",\n",
    "    \"我が国の周波数帯ごとの主な用途と電波の特徴の図は何を表していますか？\",\n",
    "    \"衛星・HAPSによる通信サービスの提供イメージの図は何を表していますか？\",\n",
    "    \"日本のデータセンターの設置状況（地域別サーバールーム面積）において100,000㎡以上の地域はどこですか？\"\n",
    "]\n",
    "\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "print(f\"=== 開始: {len(queries)}件の質問を処理します ===\\n\")\n",
    "\n",
    "for i, q in enumerate(queries, 1):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"【{i}/{len(queries)}】質問: {q}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    res = qa.answer_question(q, top_k=5, max_new_tokens=300)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(\"【回答】\")\n",
    "    print(res[\"answer\"])\n",
    "    print(f\"\\n【参照ソース】\")\n",
    "    for src in res[\"sources\"]:\n",
    "        print(f\"- {src['pdf_name']} ページ{src['page_num']} (スコア:{src['score']:.3f})\")\n",
    "    print()\n",
    "\n",
    "print(\" 全ての質問の処理が完了しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87632b0f-bfaa-4ad7-9d75-fc3d3ca4f2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
